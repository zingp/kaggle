{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import os \n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pytorch中神经网络模块化接口nn的了解:\n",
    "\n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络。\n",
    "nn.Module是nn中十分重要的类,包含网络各层的定义及forward方法。\n",
    "定义自已的网络：\n",
    "    需要继承nn.Module类，并实现forward方法。\n",
    "    一般把网络中具有可学习参数的层放在构造函数__init__()中，\n",
    "    不具有可学习参数的层(如ReLU)可放在构造函数中，也可不放在构造函数中(而在forward中使用nn.functional来代替)\n",
    "    \n",
    "    只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。\n",
    "    在forward函数中可以使用任何Variable支持的函数，毕竟在整个pytorch构建的图中，是Variable在流动。还可以使用\n",
    "    if,for,print,log等python语法.\n",
    "    \n",
    "    注：Pytorch基于nn.Module构建的模型中，只支持mini-batch的Variable输入方式，\n",
    "    比如，只有一张输入图片，也需要变成 N x C x H x W 的形式：\n",
    "    \n",
    "    input_image = torch.FloatTensor(1, 28, 28)\n",
    "    input_image = Variable(input_image)\n",
    "    input_image = input_image.unsqueeze(0)   # 1 x 1 x 28 x 28\n",
    "    \n",
    "    二维卷积层, 输入的尺度是(N, C_in,H,W)，输出尺度（N,C_out,H_out,W_out）的计算方式\n",
    "    torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "    in_channels(int) – 输入信号的通道\n",
    "    out_channels(int) – 卷积产生的通道\n",
    "    kerner_size(int or tuple) - 卷积核的尺寸\n",
    "    stride(int or tuple, optional) - 卷积步长\n",
    "    padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "    dilation(int or tuple, optional) – 卷积核元素之间的间距\n",
    "    groups(int, optional) – 从输入通道到输出通道的阻塞连接数\n",
    "    bias(bool, optional) - 如果bias=True，添加偏置\n",
    "\"\"\"\n",
    "class ShuffleBlock(nn.Module):\n",
    "    def __init__(self,groups):\n",
    "        super(ShuffleBlock,self).__init__()\n",
    "        self.groups=groups\n",
    "    def forward(self,x):#转置重组操作\n",
    "        '''\n",
    "            [N,C,H,W]->分组操作->[N,C/g,H,W]*g->转置重组->[N,g,H,W]*C/g\n",
    "        '''\n",
    "        N,C,H,W=x.size()\n",
    "        g=self.groups\n",
    "        return x.view(N,g,int(C/g),H,W).permute(0,2,1,3,4).contiguous().view(N,C,H,W)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, groups):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        mid_planes = int(out_planes/4)\n",
    "        g = 1 if in_planes==24 else groups\n",
    "        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
    "        self.shuffle1 = ShuffleBlock(groups=g)\n",
    "        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=int(mid_planes), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_planes)\n",
    "        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 2:\n",
    "            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.shuffle1(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        res = self.shortcut(x)\n",
    "        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n",
    "        return out\n",
    "    \n",
    "class ShuffleNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ShuffleNet, self).__init__()\n",
    "        out_planes = cfg['out_planes']\n",
    "        num_blocks = cfg['num_blocks']\n",
    "        groups = cfg['groups']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.in_planes = 24    # in_planes\n",
    "        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n",
    "        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n",
    "        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n",
    "        self.linear = nn.Linear(out_planes[2], 2)\n",
    "\n",
    "    def _make_layer(self, out_planes, num_blocks, groups):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = 2 if i == 0 else 1\n",
    "            cat_planes = self.in_planes if i == 0 else 0\n",
    "            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n",
    "            self.in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 模型训练与评估类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        func(*args,**kwargs)\n",
    "        end = time.time()\n",
    "        cost = end - start\n",
    "        print(\"Cost time: {} mins.\".format(cost/60)) \n",
    "    return wrapper\n",
    "\n",
    "class CNNModel(object):\n",
    "    def __init__(self, model, train_data, test_data, model_dir, model_name,\n",
    "                 best_valid_loss=float('inf'), n_split=0.9, batch_size=64, epochs=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.n_split = n_split\n",
    "        \n",
    "        self.train_data =  train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.device = self.get_device()\n",
    "        self.init_data()\n",
    "        self.init_iterator()\n",
    "        self.init_model_path()\n",
    "        \n",
    "        self.model = self.init_model(model)\n",
    "        self.optimizer = self.set_optimizer()\n",
    "        self.criterion = self.set_criterion()\n",
    "        \n",
    "    def get_device(self):\n",
    "        d = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return d\n",
    "    \n",
    "    def init_data(self):\n",
    "        n_train = int(len(self.train_data)*self.n_split)\n",
    "        n_validation = len(self.train_data) - n_train\n",
    "        self.train_data, self.valid_data = torch.utils.data.random_split(self.train_data, [n_train, n_validation])\n",
    "    \n",
    "    def init_iterator(self):\n",
    "        self.train_iterator = torch.utils.data.DataLoader(self.train_data, shuffle=True, batch_size=self.batch_size)\n",
    "        self.valid_iterator = torch.utils.data.DataLoader(self.valid_data, batch_size=self.batch_size)\n",
    "        self.test_iterator = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size)\n",
    "        \n",
    "    def set_optimizer(self):\n",
    "        optimizer = optim.Adam(self.model.parameters()) \n",
    "        return optimizer\n",
    "    \n",
    "    def set_criterion(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion\n",
    "    \n",
    "    def init_model(self, model):\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(self.device)\n",
    "        return model\n",
    "        \n",
    "    def init_model_path(self):\n",
    "        if not os.path.isdir(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.model_path = os.path.join(self.model_dir, self.model_name)\n",
    "        \n",
    "    # 定义评估函数\n",
    "    def accu(self, fx, y):\n",
    "        pred = fx.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum()  # 得到该batch的准确度\n",
    "        acc = correct.float()/pred.shape[0]\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        epoch_loss = 0   # 积累变量\n",
    "        epoch_acc = 0    # 积累变量\n",
    "        self.model.train()    # 该函数表示PHASE=Train\n",
    "\n",
    "        for (x,y) in self.train_iterator:  # 拿去每一个minibatch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            fx = self.model(x)           # 进行forward\n",
    "            loss = self.criterion(fx,y)  # 计算Loss,train_loss\n",
    "            type(loss)\n",
    "            acc = self.accu(fx,y)    # 计算精确度，train_accu\n",
    "            loss.backward()          # 进行BP\n",
    "            self.optimizer.step()    # 统一更新模型\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss/len(self.train_iterator),epoch_acc/len(self.train_iterator)\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (x,y) in iterator:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                fx = self.model(x)\n",
    "                loss = self.criterion(fx,y)\n",
    "                acc = self.accu(fx,y)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "    \n",
    "    @timer\n",
    "    def train_fit(self):\n",
    "        info = 'Epoch:{0} | Train Loss:{1} | Train Acc:{2} | Val Loss:{3} | Val Acc:{4}'\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_acc = self.train()\n",
    "            valid_loss, valid_acc = self.evaluate(self.valid_iterator)\n",
    "            if valid_loss < self.best_valid_loss:  # 如果是最好的模型就保存到文件夹\n",
    "                self.best_valid_loss = valid_loss\n",
    "                torch.save(self.model.state_dict(), self.model_path)\n",
    "            print(info.format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "    \n",
    "    def get_acc(self):\n",
    "        self.model.load_state_dict(torch.load(self.model_path))\n",
    "        test_loss, test_acc = self.evaluate(self.test_iterator)\n",
    "        print('| Test Loss: {0} | Test Acc: {1} |'.format(test_loss,test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 数据集的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_filenames = os.listdir('data/train')\n",
    "datasetdir = os.path.join('./data')\n",
    "traindir = os.path.join(datasetdir,'train2')\n",
    "testdir = os.path.join(datasetdir,'test2')\n",
    "\n",
    "batch_size = 24\n",
    "epochs = 10\n",
    "csv_name = \"submission.csv\"\n",
    "trick = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = filter(lambda x:x[:3]=='cat', tran_filenames)\n",
    "train_dog = filter(lambda x:x[:3]=='dog', tran_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "train_dataset = datasets.ImageFolder(traindir,transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 随机挑一些图镜像翻转\n",
    "    transforms.RandomResizedCrop(32),  #224\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "]))\n",
    "\n",
    "test_dataset = datasets.ImageFolder(testdir, transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "        transforms.CenterCrop(32),   #224\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 25000\n",
       "    Root Location: ./data/train2\n",
       "    Transforms (if any): Compose(\n",
       "                             RandomHorizontalFlip(p=0.5)\n",
       "                             RandomResizedCrop(size=(32, 32), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "                             RandomHorizontalFlip(p=0.5)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "n_split = 0.9\n",
    "batch_size = 32\n",
    "model_dir = 'models'\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "model_name = \"shufflenet_c2.pt\"\n",
    "\n",
    "cfg={\n",
    "    'out_planes':[200, 400, 800],\n",
    "    'num_blocks':[4, 8, 4],\n",
    "    'groups': 2\n",
    "}\n",
    "\n",
    "model = ShuffleNet(cfg)\n",
    "\n",
    "obj = CNNModel(model=model, \n",
    "               train_data=train_dataset, \n",
    "               test_data=test_dataset, \n",
    "               model_dir=model_dir, \n",
    "               model_name=model_name,\n",
    "               best_valid_loss=best_valid_loss, \n",
    "               n_split=n_split, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, parameters in model.named_parameters():   # 各层参数及具体数字\n",
    "#     print('name: {}, param: {}'.format(name, parameters))\n",
    "# for n, c in model.named_children():    # 各层名称与具体定义\n",
    "#     print(\"name:{}, children:{}\".format(n,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ShuffleNet(\n",
      "    (conv1): Conv2d(3, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(24, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=44, bias=False)\n",
      "        (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(44, 176, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50, bias=False)\n",
      "        (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50, bias=False)\n",
      "        (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50, bias=False)\n",
      "        (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50, bias=False)\n",
      "        (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=100, bias=False)\n",
      "        (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(800, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "        (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(200, 800, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(800, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "        (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(200, 800, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(800, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shuffle1): ShuffleBlock()\n",
      "        (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "        (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(200, 800, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
      "        (bn3): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=800, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Train Loss:0.6817506957291202 | Train Acc:0.5847833806818182 | Val Loss:0.7120145913920825 | Val Acc:0.6127373417721519\n",
      "Epoch:2 | Train Loss:0.650565601842986 | Train Acc:0.6271306818181818 | Val Loss:0.6404438173469109 | Val Acc:0.6348892405063291\n",
      "Epoch:3 | Train Loss:0.6227682635865428 | Train Acc:0.6579811789772727 | Val Loss:0.5908296904231929 | Val Acc:0.6878955696202531\n",
      "Epoch:4 | Train Loss:0.6038446096618745 | Train Acc:0.6721857244318182 | Val Loss:0.5807541291924971 | Val Acc:0.6910601265822784\n",
      "Epoch:5 | Train Loss:0.5822103642011908 | Train Acc:0.6909623579545454 | Val Loss:0.5492817755741409 | Val Acc:0.7143987341772152\n",
      "Epoch:6 | Train Loss:0.5616090063598346 | Train Acc:0.7079190340909091 | Val Loss:0.5735848044292836 | Val Acc:0.7108386075949367\n",
      "Epoch:7 | Train Loss:0.5472854843617163 | Train Acc:0.7197265625 | Val Loss:0.5536691878415361 | Val Acc:0.7203322784810127\n",
      "Epoch:8 | Train Loss:0.5318808886222541 | Train Acc:0.7310014204545454 | Val Loss:0.5485694065124174 | Val Acc:0.721123417721519\n",
      "Epoch:9 | Train Loss:0.5106614199890331 | Train Acc:0.74462890625 | Val Loss:0.5469222668605515 | Val Acc:0.7266613924050633\n",
      "Epoch:10 | Train Loss:0.4981004605493085 | Train Acc:0.7542169744318182 | Val Loss:0.47563317531271826 | Val Acc:0.7709651898734177\n",
      "Epoch:11 | Train Loss:0.4857427570333874 | Train Acc:0.7614524147727273 | Val Loss:0.4972953605878202 | Val Acc:0.747626582278481\n",
      "Epoch:12 | Train Loss:0.46575477721423586 | Train Acc:0.7716619318181818 | Val Loss:0.4594257898345778 | Val Acc:0.774129746835443\n",
      "Epoch:13 | Train Loss:0.4510342237666588 | Train Acc:0.7840465198863636 | Val Loss:0.43710647127296354 | Val Acc:0.7891613924050633\n",
      "Epoch:14 | Train Loss:0.44045035606673494 | Train Acc:0.7867986505681818 | Val Loss:0.4109598219017439 | Val Acc:0.8010284810126582\n",
      "Epoch:15 | Train Loss:0.43848610087297857 | Train Acc:0.7878639914772727 | Val Loss:0.41207665110690683 | Val Acc:0.8105221518987342\n",
      "Epoch:16 | Train Loss:0.4273434796963226 | Train Acc:0.7958984375 | Val Loss:0.40456506459018854 | Val Acc:0.8121044303797469\n",
      "Epoch:17 | Train Loss:0.41463644021529367 | Train Acc:0.8019797585227273 | Val Loss:0.4379015672433225 | Val Acc:0.7950949367088608\n",
      "Epoch:18 | Train Loss:0.40822946963916446 | Train Acc:0.8040216619318182 | Val Loss:0.39045102271852616 | Val Acc:0.8113132911392406\n",
      "Epoch:19 | Train Loss:0.397136209257455 | Train Acc:0.8140092329545454 | Val Loss:0.3658442938629585 | Val Acc:0.8295094936708861\n",
      "Epoch:20 | Train Loss:0.3860859704119238 | Train Acc:0.818359375 | Val Loss:0.39935793680480763 | Val Acc:0.8081487341772152\n",
      "Epoch:21 | Train Loss:0.3828829622361809 | Train Acc:0.8199573863636364 | Val Loss:0.3791789414384697 | Val Acc:0.814873417721519\n",
      "Epoch:22 | Train Loss:0.3781380029818551 | Train Acc:0.8208895596590909 | Val Loss:0.37235992958274067 | Val Acc:0.8291139240506329\n",
      "Epoch:23 | Train Loss:0.36570052739063447 | Train Acc:0.8311434659090909 | Val Loss:0.37200157721585864 | Val Acc:0.8247626582278481\n",
      "Epoch:24 | Train Loss:0.36154449500397523 | Train Acc:0.8299005681818182 | Val Loss:0.3663597178610065 | Val Acc:0.8283227848101266\n",
      "Epoch:25 | Train Loss:0.3579310322053392 | Train Acc:0.8320756392045454 | Val Loss:0.3493458864432347 | Val Acc:0.8291139240506329\n",
      "Epoch:26 | Train Loss:0.35067286727611313 | Train Acc:0.8350053267045454 | Val Loss:0.33528598463988 | Val Acc:0.8465189873417721\n",
      "Epoch:27 | Train Loss:0.3414462575138631 | Train Acc:0.84228515625 | Val Loss:0.3350574298749996 | Val Acc:0.8477056962025317\n",
      "Epoch:28 | Train Loss:0.3399526254189285 | Train Acc:0.8421963778409091 | Val Loss:0.356512705453589 | Val Acc:0.8370253164556962\n",
      "Epoch:29 | Train Loss:0.3382533718620173 | Train Acc:0.8447709517045454 | Val Loss:0.35366454371545886 | Val Acc:0.8393987341772152\n",
      "Epoch:30 | Train Loss:0.3355894997631284 | Train Acc:0.8418412642045454 | Val Loss:0.32969814727577984 | Val Acc:0.8453322784810127\n",
      "Epoch:31 | Train Loss:0.3285778803031214 | Train Acc:0.8492542613636364 | Val Loss:0.32817284253579154 | Val Acc:0.8504746835443038\n",
      "Epoch:32 | Train Loss:0.3218533414606513 | Train Acc:0.8497869318181818 | Val Loss:0.3101764426955694 | Val Acc:0.8579905063291139\n",
      "Epoch:33 | Train Loss:0.31553485431835393 | Train Acc:0.8528497869318182 | Val Loss:0.33445673784877683 | Val Acc:0.8500791139240507\n",
      "Epoch:34 | Train Loss:0.321256224764511 | Train Acc:0.8502752130681818 | Val Loss:0.3185531258394447 | Val Acc:0.8560126582278481\n",
      "Epoch:35 | Train Loss:0.3113865320232104 | Train Acc:0.8587091619318182 | Val Loss:0.30997917895452887 | Val Acc:0.8540348101265823\n",
      "Epoch:36 | Train Loss:0.3048301233634861 | Train Acc:0.8597301136363636 | Val Loss:0.3053090628000754 | Val Acc:0.8591772151898734\n",
      "Epoch:37 | Train Loss:0.30558444518299605 | Train Acc:0.8623934659090909 | Val Loss:0.3236763028404381 | Val Acc:0.8571993670886076\n",
      "Epoch:38 | Train Loss:0.2983686944617974 | Train Acc:0.8631036931818182 | Val Loss:0.29596504691658143 | Val Acc:0.8651107594936709\n",
      "Epoch:39 | Train Loss:0.2960508477620103 | Train Acc:0.8624378551136364 | Val Loss:0.2978232610452024 | Val Acc:0.8651107594936709\n",
      "Epoch:40 | Train Loss:0.29287615236402914 | Train Acc:0.8673206676136364 | Val Loss:0.2987956233039687 | Val Acc:0.8643196202531646\n",
      "Epoch:41 | Train Loss:0.29586715742268344 | Train Acc:0.8637251420454546 | Val Loss:0.2837344418409504 | Val Acc:0.8738132911392406\n",
      "Epoch:42 | Train Loss:0.2935498064828359 | Train Acc:0.8651455965909091 | Val Loss:0.2976489534860925 | Val Acc:0.8607594936708861\n",
      "Epoch:43 | Train Loss:0.28737221302633936 | Train Acc:0.8679865056818182 | Val Loss:0.3014418163065669 | Val Acc:0.8627373417721519\n",
      "Epoch:44 | Train Loss:0.2880979733520441 | Train Acc:0.8656338778409091 | Val Loss:0.2656708368206326 | Val Acc:0.8813291139240507\n",
      "Epoch:45 | Train Loss:0.28198113899931987 | Train Acc:0.8690518465909091 | Val Loss:0.2851477214429952 | Val Acc:0.8706487341772152\n",
      "Epoch:46 | Train Loss:0.28041491497689014 | Train Acc:0.8711825284090909 | Val Loss:0.28885080071189734 | Val Acc:0.870253164556962\n",
      "Epoch:47 | Train Loss:0.27678958009081805 | Train Acc:0.8729580965909091 | Val Loss:0.2682554876502556 | Val Acc:0.8734177215189873\n",
      "Epoch:48 | Train Loss:0.27091325112533843 | Train Acc:0.8763760653409091 | Val Loss:0.2800525789019428 | Val Acc:0.8686708860759493\n",
      "Epoch:49 | Train Loss:0.2740769547897137 | Train Acc:0.8757102272727273 | Val Loss:0.2970487562161458 | Val Acc:0.8639240506329114\n",
      "Epoch:50 | Train Loss:0.2723617666210471 | Train Acc:0.875 | Val Loss:0.27094911557586887 | Val Acc:0.8742088607594937\n",
      "Epoch:51 | Train Loss:0.271040953992104 | Train Acc:0.8765980113636364 | Val Loss:0.2728624691880202 | Val Acc:0.8801424050632911\n",
      "Epoch:52 | Train Loss:0.26666334869382396 | Train Acc:0.8776633522727273 | Val Loss:0.30531203426137754 | Val Acc:0.8615506329113924\n",
      "Epoch:53 | Train Loss:0.2607616991206834 | Train Acc:0.8810369318181818 | Val Loss:0.2800875822011429 | Val Acc:0.879746835443038\n",
      "Epoch:54 | Train Loss:0.2657856154467233 | Train Acc:0.8780184659090909 | Val Loss:0.28456370617392696 | Val Acc:0.8666930379746836\n",
      "Epoch:55 | Train Loss:0.2638492026599124 | Train Acc:0.8785067471590909 | Val Loss:0.2877738752131221 | Val Acc:0.8659018987341772\n",
      "Epoch:56 | Train Loss:0.25785297724757006 | Train Acc:0.8836115056818182 | Val Loss:0.2645869997294643 | Val Acc:0.8817246835443038\n",
      "Epoch:57 | Train Loss:0.26193718934981997 | Train Acc:0.8791281960227273 | Val Loss:0.2888540520415276 | Val Acc:0.8722310126582279\n",
      "Epoch:58 | Train Loss:0.25141707506157795 | Train Acc:0.8870738636363636 | Val Loss:0.2966983627669419 | Val Acc:0.8765822784810127\n",
      "Epoch:59 | Train Loss:0.25737389173909003 | Train Acc:0.8837446732954546 | Val Loss:0.2742487155372583 | Val Acc:0.8698575949367089\n",
      "Epoch:60 | Train Loss:0.24832313890907576 | Train Acc:0.8863636363636364 | Val Loss:0.2893284277259549 | Val Acc:0.8714398734177216\n",
      "Epoch:61 | Train Loss:0.25557025478602474 | Train Acc:0.8832120028409091 | Val Loss:0.283583409046825 | Val Acc:0.8714398734177216\n",
      "Epoch:62 | Train Loss:0.2501355227114717 | Train Acc:0.8845880681818182 | Val Loss:0.28908784366861173 | Val Acc:0.8666930379746836\n",
      "Epoch:63 | Train Loss:0.24615186257076196 | Train Acc:0.8892045454545454 | Val Loss:0.2864758914595918 | Val Acc:0.8817246835443038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:64 | Train Loss:0.2470878683115271 | Train Acc:0.8873401988636364 | Val Loss:0.26485199712311164 | Val Acc:0.8801424050632911\n",
      "Epoch:65 | Train Loss:0.2494010366446508 | Train Acc:0.8873401988636364 | Val Loss:0.26215022802352905 | Val Acc:0.8900316455696202\n",
      "Epoch:66 | Train Loss:0.2423110963692042 | Train Acc:0.8915127840909091 | Val Loss:0.2587999675092818 | Val Acc:0.8840981012658228\n",
      "Epoch:67 | Train Loss:0.24082251709610733 | Train Acc:0.8918678977272727 | Val Loss:0.27906003056824963 | Val Acc:0.8742088607594937\n",
      "Epoch:68 | Train Loss:0.24317655877464198 | Train Acc:0.8916459517045454 | Val Loss:0.2606967370721358 | Val Acc:0.884493670886076\n",
      "Epoch:69 | Train Loss:0.24142923661698165 | Train Acc:0.8896484375 | Val Loss:0.25895260717672636 | Val Acc:0.8777689873417721\n",
      "Epoch:70 | Train Loss:0.2367617966109802 | Train Acc:0.8937766335227273 | Val Loss:0.2669787236218211 | Val Acc:0.877373417721519\n",
      "Epoch:71 | Train Loss:0.23857786287871105 | Train Acc:0.8902698863636364 | Val Loss:0.28650365901898733 | Val Acc:0.8746044303797469\n",
      "Epoch:72 | Train Loss:0.237493753681933 | Train Acc:0.8915571732954546 | Val Loss:0.2643169256139405 | Val Acc:0.8888449367088608\n",
      "Epoch:73 | Train Loss:0.23461248161567544 | Train Acc:0.8914240056818182 | Val Loss:0.24988809341116797 | Val Acc:0.8888449367088608\n",
      "Epoch:74 | Train Loss:0.23333282168658281 | Train Acc:0.8957741477272727 | Val Loss:0.25810299832609634 | Val Acc:0.877373417721519\n",
      "Epoch:75 | Train Loss:0.23153053527824918 | Train Acc:0.89697265625 | Val Loss:0.2750344940378696 | Val Acc:0.8805379746835443\n",
      "Epoch:76 | Train Loss:0.2279177083983086 | Train Acc:0.89794921875 | Val Loss:0.24555656144136115 | Val Acc:0.8920094936708861\n",
      "Epoch:77 | Train Loss:0.23071715607180854 | Train Acc:0.8968394886363636 | Val Loss:0.27752346187075483 | Val Acc:0.8746044303797469\n",
      "Epoch:78 | Train Loss:0.22782572894357145 | Train Acc:0.8963955965909091 | Val Loss:0.2382022254640543 | Val Acc:0.8951740506329114\n",
      "Epoch:79 | Train Loss:0.23009277796584435 | Train Acc:0.8967063210227273 | Val Loss:0.24917163869625406 | Val Acc:0.8924050632911392\n",
      "Epoch:80 | Train Loss:0.22669249964052474 | Train Acc:0.8987038352272727 | Val Loss:0.250028538175776 | Val Acc:0.8880537974683544\n",
      "Epoch:81 | Train Loss:0.22940088346579365 | Train Acc:0.8974165482954546 | Val Loss:0.2334647115461434 | Val Acc:0.8943829113924051\n",
      "Epoch:82 | Train Loss:0.22507050966272468 | Train Acc:0.8993696732954546 | Val Loss:0.24242852335866494 | Val Acc:0.8904272151898734\n",
      "Epoch:83 | Train Loss:0.22238722729856486 | Train Acc:0.9003018465909091 | Val Loss:0.2662153629751145 | Val Acc:0.8829113924050633\n",
      "Epoch:84 | Train Loss:0.223990946309641 | Train Acc:0.8989701704545454 | Val Loss:0.2500522798752483 | Val Acc:0.882120253164557\n",
      "Epoch:85 | Train Loss:0.21810982181076807 | Train Acc:0.9005681818181818 | Val Loss:0.24099848480730118 | Val Acc:0.8860759493670886\n",
      "Epoch:86 | Train Loss:0.21921836940931494 | Train Acc:0.9036754261363636 | Val Loss:0.2250641891379145 | Val Acc:0.9030854430379747\n",
      "Epoch:87 | Train Loss:0.22261630617801778 | Train Acc:0.9018110795454546 | Val Loss:0.2915798215360581 | Val Acc:0.8714398734177216\n",
      "Epoch:88 | Train Loss:0.21721711332528768 | Train Acc:0.9042524857954546 | Val Loss:0.2375392794043203 | Val Acc:0.8975474683544303\n",
      "Epoch:89 | Train Loss:0.21511930199234153 | Train Acc:0.9033647017045454 | Val Loss:0.25901016385494907 | Val Acc:0.8856803797468354\n",
      "Epoch:90 | Train Loss:0.218133565408855 | Train Acc:0.9021218039772727 | Val Loss:0.22764954046358035 | Val Acc:0.896756329113924\n",
      "Epoch:91 | Train Loss:0.2138729412267408 | Train Acc:0.9011008522727273 | Val Loss:0.2506651504884792 | Val Acc:0.8868670886075949\n",
      "Epoch:92 | Train Loss:0.21753470101182096 | Train Acc:0.9037198153409091 | Val Loss:0.24780680776774128 | Val Acc:0.8920094936708861\n",
      "Epoch:93 | Train Loss:0.21133794825502925 | Train Acc:0.9045188210227273 | Val Loss:0.2303495442942728 | Val Acc:0.901503164556962\n",
      "Epoch:94 | Train Loss:0.21597371720285577 | Train Acc:0.8996360085227273 | Val Loss:0.2248885830557799 | Val Acc:0.8975474683544303\n",
      "Epoch:95 | Train Loss:0.2146119814450768 | Train Acc:0.9037642045454546 | Val Loss:0.2264541790738136 | Val Acc:0.8935917721518988\n",
      "Epoch:96 | Train Loss:0.2124309054395946 | Train Acc:0.9028764204545454 | Val Loss:0.2440655575711516 | Val Acc:0.8963607594936709\n",
      "Epoch:97 | Train Loss:0.21007089454955843 | Train Acc:0.9065607244318182 | Val Loss:0.22195359394897388 | Val Acc:0.9054588607594937\n",
      "Epoch:98 | Train Loss:0.21169104622359472 | Train Acc:0.90576171875 | Val Loss:0.25335519859873795 | Val Acc:0.8884493670886076\n",
      "Epoch:99 | Train Loss:0.21499165773539888 | Train Acc:0.9052290482954546 | Val Loss:0.22466289492536196 | Val Acc:0.9007120253164557\n",
      "Epoch:100 | Train Loss:0.21050817500905727 | Train Acc:0.9063387784090909 | Val Loss:0.23969292376614823 | Val Acc:0.8943829113924051\n",
      "Cost time: 342.435641570886 mins.\n"
     ]
    }
   ],
   "source": [
    "obj.train_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.get_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss的内部是怎么计算的？\n",
    "# acc的计算 8/64 8/64  16/128=(8/64+8/64)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, loader):#用来对test数据集进行predict的函数\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    for x, _ in loader:\n",
    "        with torch.no_grad():\n",
    "            x_var = x.cuda()\n",
    "            scores = model(x_var)\n",
    "            preds.append(scores)\n",
    "    print('duration = %s\\n' % timedelta(seconds=time.time() - start_time))\n",
    "    return preds\n",
    "\n",
    "def save_csv(preds, images, csv_name, clib=False):\n",
    "    pred_soft = F.softmax(torch.cat(preds), dim=1).cpu().numpy()\n",
    "    if clib:\n",
    "        pred_soft = pred_soft.clip(min=0.005,max=0.995)\n",
    "    pred_result = pred_soft[:, 1]\n",
    "    results = zip(images, pred_result)\n",
    "    idx = [(x[0].split('/')[-1]).split('.')[0] for x in results]\n",
    "    #labels = [0 if x[1]<0.5 else 1 for x in results]\n",
    "    res = pd.DataFrame.from_dict({\n",
    "        'id': idx,\n",
    "        'label': pred_result.tolist()\n",
    "    })\n",
    "    res = res.set_index('id')\n",
    "    res.to_csv(csv_name)\n",
    "    print(\"Save {} done.\".format(csv_name))\n",
    "    \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "shuffle_model = ShuffleNet(cfg)\n",
    "shuffle_model = shuffle_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [os.path.join(testdir, 'test', f) for f in sorted(os.listdir(os.path.join(testdir,'test')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration = 0:00:26.489227\n",
      "\n",
      "Save submission.csv done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "preds = run_test(obj.model, test_loader)\n",
    "save_csv(preds, test_images, csv_name, clib=trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save submission.csv done.\n"
     ]
    }
   ],
   "source": [
    "save_csv(preds, test_images, csv_name, clib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
